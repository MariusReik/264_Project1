{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 â€“ Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "Imports og random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from decision_tree import DecisionTree\n",
    "from random_forest import RandomForest\n",
    "\n",
    "# random seed (bare satt en tilfeldig verdi)\n",
    "np.random.seed(21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "Laster inn letters.csv og deler opp i features (X) og labels (y), og deler inn i test/train datasett (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"letters.csv\", delimiter=\",\", dtype=float, names=True)\n",
    "\n",
    "feature_names = list(data.dtype.names[:-1])\n",
    "target_name = data.dtype.names[-1]\n",
    "\n",
    "X = np.array([data[f] for f in feature_names]).T\n",
    "y = data[target_name].astype(int)\n",
    "\n",
    "# 80/20 train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "print(data[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparametere\n",
    "\n",
    "Setter opp verdier som skal testes i grid search for DecisionTree og RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "    \"max_depth\": [None, 5, 10, 20],\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [10, 20, 40],\n",
    "    \"max_depth\": [5, 10, None],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "Funksjon for k-fold cross validation som regner ut accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_score_custom(model_class, params, X, y, k=5, seed=21):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = model_class(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "    return float(np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree grid search\n",
    "\n",
    "Tester kombinasjoner av hyperparametere og finner beste for DecisionTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt_params = None\n",
    "best_dt_score = -1.0\n",
    "\n",
    "for criterion, max_depth, max_features in product(\n",
    "    dt_params[\"criterion\"], dt_params[\"max_depth\"], dt_params[\"max_features\"]\n",
    "):\n",
    "    params = {\n",
    "        \"criterion\": criterion,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"max_features\": max_features,\n",
    "    }\n",
    "    score = cross_val_score_custom(DecisionTree, params, X_train, y_train, k=5, seed=21)\n",
    "    if score > best_dt_score:\n",
    "        best_dt_score = score\n",
    "        best_dt_params = params\n",
    "\n",
    "print(\"Best DecisionTree params:\", best_dt_params)\n",
    "print(\"Best DecisionTree 5-fold CV accuracy:\", round(best_dt_score, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForset - grid Serach\n",
    "Tester kombinasjoner av hyperparametere og finner beste for RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_params = None\n",
    "best_rf_score = -1.0\n",
    "\n",
    "for n_estimators, max_depth, criterion, max_features in product(\n",
    "    rf_params[\"n_estimators\"],\n",
    "    rf_params[\"max_depth\"],\n",
    "    rf_params[\"criterion\"],\n",
    "    rf_params[\"max_features\"],\n",
    "):\n",
    "    params = {\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"criterion\": criterion,\n",
    "        \"max_features\": max_features,\n",
    "    }\n",
    "    score = cross_val_score_custom(RandomForest, params, X_train, y_train, k=5, seed=21)\n",
    "    if score > best_rf_score:\n",
    "        best_rf_score = score\n",
    "        best_rf_params = params\n",
    "\n",
    "print(\"Best RandomForest params:\", best_rf_params)\n",
    "print(\"Best RandomForest 5-fold CV accuracy:\", round(best_rf_score, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree accuracy vs max_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1, 2, 5, 10, 20, None]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for d in depths:\n",
    "    model = DecisionTree(max_depth=d, criterion=\"gini\", max_features=None)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_scores.append(accuracy_score(y_train, model.predict(X_train)))\n",
    "    test_scores.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "plt.plot([str(d) for d in depths], train_scores, marker=\"o\", label=\"Train\")\n",
    "plt.plot([str(d) for d in depths], test_scores, marker=\"o\", label=\"Test\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"DecisionTree accuracy vs max_depth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest accuracy vs n_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [1, 5, 10, 20, 50, 100]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n in estimators:\n",
    "    model = RandomForest(n_estimators=n, max_depth=10, criterion=\"gini\", max_features=\"sqrt\")\n",
    "    model.fit(X_train, y_train)\n",
    "    train_scores.append(accuracy_score(y_train, model.predict(X_train)))\n",
    "    test_scores.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "plt.plot(estimators, train_scores, marker=\"o\", label=\"Train\")\n",
    "plt.plot(estimators, test_scores, marker=\"o\", label=\"Test\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"RandomForest accuracy vs n_estimators\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation\n",
    "\n",
    "We retrain our models with the best hyperparameters on the full training set and evaluate on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree\n",
    "dt_best = DecisionTree(**best_dt_params)\n",
    "dt_best.fit(X_train, y_train)\n",
    "dt_pred = dt_best.predict(X_test)\n",
    "dt_acc = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# RandomForest\n",
    "rf_best = RandomForest(**best_rf_params)\n",
    "rf_best.fit(X_train, y_train)\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Custom DecisionTree test accuracy: {dt_acc:.4f}\")\n",
    "print(f\"Custom RandomForest test accuracy: {rf_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn modeller\n",
    "\n",
    "Trener modell med sklearn med de samme paramterene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn DecisionTree\n",
    "sk_dt = DecisionTreeClassifier(\n",
    "    criterion=best_dt_params[\"criterion\"],\n",
    "    max_depth=best_dt_params[\"max_depth\"],\n",
    "    max_features=best_dt_params[\"max_features\"],\n",
    "    random_state=0,\n",
    ")\n",
    "sk_dt.fit(X_train, y_train)\n",
    "sk_dt_acc = accuracy_score(y_test, sk_dt.predicx    t(X_test))\n",
    "\n",
    "# Sklearn RandomForest\n",
    "sk_rf = RandomForestClassifier(\n",
    "    n_estimators=best_rf_params[\"n_estimators\"],\n",
    "    max_depth=best_rf_params[\"max_depth\"],\n",
    "    criterion=best_rf_params[\"criterion\"],\n",
    "    max_features=best_rf_params[\"max_features\"],\n",
    "    random_state=0,\n",
    ")\n",
    "sk_rf.fit(X_train, y_train)\n",
    "sk_rf_acc = accuracy_score(y_test, sk_rf.predict(X_test))\n",
    "\n",
    "print(\"Sklearn DecisionTree test accuracy:\", round(sk_dt_acc, 4))\n",
    "print(\"Sklearn RandomForest test accuracy:\", round(sk_rf_acc, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4 - Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance(model, X, y, metric=accuracy_score, n_repeats=30, seed=21):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    baseline = metric(y, model.predict(X))\n",
    "    importances = []\n",
    "\n",
    "    for col in range(X.shape[1]):\n",
    "        scores = []\n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = X.copy()\n",
    "            rng.shuffle(X_permuted[:, col])\n",
    "            score = metric(y, model.predict(X_permuted))\n",
    "            scores.append(baseline - score)\n",
    "        importances.append(np.mean(scores))\n",
    "    return np.array(importances)\n",
    "\n",
    "rf_best = RandomForest(**best_rf_params)\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "importances = permutation_importance(rf_best, X_test, y_test, n_repeats=30, seed=21)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(len(feature_names)), importances)\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=90)\n",
    "plt.ylabel(\"Permutation importance\")\n",
    "plt.title(\"Feature importance RandomForest\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (INF264)",
   "language": "python",
   "name": "inf264"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
